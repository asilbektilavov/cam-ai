import { GoogleGenerativeAI } from '@google/generative-ai';
import { readFile } from 'fs/promises';
import { prisma } from '@/lib/prisma';
import { getFrameAbsolutePath } from './frame-storage';
import { appEvents, CameraEvent } from './event-emitter';
import { smartFeaturesEngine } from './smart-features-engine';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');

const BASE_PROMPT = `You are a security camera AI analyst. Analyze this surveillance camera frame and respond ONLY with valid JSON (no markdown, no code blocks).

JSON format:
{
  "description": "Brief description of what's happening in the scene (in Russian)",
  "peopleCount": 0,
  "objects": ["list", "of", "notable", "objects"],
  "alerts": []
}

For "alerts", include objects with format: {"type": "alert_type", "severity": "info|warning|critical", "message": "description in Russian"}

Alert types: "intrusion" (unauthorized access), "crowd" (too many people), "abandoned_object", "unusual_behavior", "safety_hazard"

Only include alerts if something genuinely concerning is visible. Be concise.`;

interface AnalysisResult {
  description: string;
  peopleCount: number;
  objects: string[];
  alerts: Array<{
    type: string;
    severity: string;
    message: string;
  }>;
  // Smart feature fields
  queueLength?: number;
  loiteringDetected?: boolean;
  loiteringDetails?: string;
  staffCount?: number;
}

async function buildPrompt(cameraId: string): Promise<string> {
  const features = await smartFeaturesEngine.getActiveFeatures(cameraId);
  if (features.length === 0) return BASE_PROMPT;

  let prompt = BASE_PROMPT;
  const extraFields: string[] = [];

  for (const feature of features) {
    switch (feature.featureType) {
      case 'queue_monitor':
        prompt += `\n\nQUEUE MONITORING: This camera monitors a checkout/service area. Count the exact number of people standing in a queue or waiting line. Report this as "queueLength" in your JSON response. If no queue is visible, set to 0.`;
        extraFields.push('"queueLength": <number of people in queue>');
        break;

      case 'loitering_detection':
        prompt += `\n\nLOITERING DETECTION: Watch for people who appear to be lingering, standing idle, or staying in one spot without clear purpose for an extended time. If someone appears to be loitering, set "loiteringDetected" to true and describe the behavior in "loiteringDetails" (in Russian).`;
        extraFields.push('"loiteringDetected": <true/false>');
        extraFields.push('"loiteringDetails": "<description if detected, in Russian>"');
        break;

      case 'workstation_monitor':
        prompt += `\n\nWORKSTATION MONITORING: This camera monitors a workstation, counter, or desk that must be staffed. Count the number of staff/workers actively present at the station (not customers/visitors). Report as "staffCount".`;
        extraFields.push('"staffCount": <number of staff at the workstation>');
        break;
    }
  }

  if (extraFields.length > 0) {
    prompt += `\n\nIMPORTANT: Include these additional fields in your JSON response:\n${extraFields.join('\n')}`;
  }

  return prompt;
}

export async function analyzeFrame(
  frameId: string,
  framePath: string,
  cameraId: string,
  organizationId: string,
  branchId: string,
  sessionId: string
): Promise<void> {
  if (!process.env.GEMINI_API_KEY) {
    console.warn('[AI] GEMINI_API_KEY not set, skipping analysis');
    return;
  }

  try {
    const absolutePath = getFrameAbsolutePath(framePath);
    const imageBuffer = await readFile(absolutePath);
    const base64Image = imageBuffer.toString('base64');

    // Build dynamic prompt based on active smart features
    const prompt = await buildPrompt(cameraId);

    const model = genAI.getGenerativeModel({ model: 'gemini-1.5-flash' });

    const result = await model.generateContent([
      prompt,
      {
        inlineData: {
          mimeType: 'image/jpeg',
          data: base64Image,
        },
      },
    ]);

    const responseText = result.response.text();

    // Parse JSON from response (handle potential markdown code blocks)
    let jsonStr = responseText;
    const jsonMatch = responseText.match(/```(?:json)?\s*([\s\S]*?)```/);
    if (jsonMatch) {
      jsonStr = jsonMatch[1].trim();
    }

    const analysis: AnalysisResult = JSON.parse(jsonStr);

    // Update the frame record
    await prisma.analysisFrame.update({
      where: { id: frameId },
      data: {
        aiResponse: responseText,
        description: analysis.description,
        peopleCount: analysis.peopleCount,
        objects: JSON.stringify(analysis.objects),
      },
    });

    // Create events for any alerts
    for (const alert of analysis.alerts) {
      await prisma.event.create({
        data: {
          cameraId,
          organizationId,
          branchId,
          type: alert.type,
          severity: alert.severity,
          description: alert.message,
          sessionId,
        },
      });

      const event: CameraEvent = {
        type: 'alert',
        cameraId,
        organizationId,
        branchId,
        data: {
          alertType: alert.type,
          severity: alert.severity,
          message: alert.message,
          sessionId,
        },
      };
      appEvents.emit('camera-event', event);
    }

    // Emit frame analyzed event
    const event: CameraEvent = {
      type: 'frame_analyzed',
      cameraId,
      organizationId,
      branchId,
      data: {
        frameId,
        description: analysis.description,
        peopleCount: analysis.peopleCount,
        sessionId,
      },
    };
    appEvents.emit('camera-event', event);

    // Evaluate smart features with the analysis results
    const camera = await prisma.camera.findUnique({
      where: { id: cameraId },
      select: { name: true, location: true },
    });

    if (camera) {
      void smartFeaturesEngine.evaluate(
        cameraId,
        organizationId,
        camera.name,
        camera.location,
        {
          peopleCount: analysis.peopleCount,
          description: analysis.description,
          queueLength: analysis.queueLength,
          loiteringDetected: analysis.loiteringDetected,
          loiteringDetails: analysis.loiteringDetails,
          staffCount: analysis.staffCount,
        }
      );
    }
  } catch (error) {
    console.error(`[AI] Analysis failed for frame ${frameId}:`, error);
  }
}
